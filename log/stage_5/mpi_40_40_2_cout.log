Sender: LSF System <lsfadmin@polus-c2-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1319140: <mpi_40_40_2> in cluster <MSUCluster> Done

Job <mpi_40_40_2> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-skmodel25-619-2-08> in cluster <MSUCluster> at Wed Oct 22 04:26:12 2025
Job was executed on host(s) <2*polus-c2-ib.bmc.hpc.cs.msu.ru>, in queue <short>, as user <edu-cmc-skmodel25-619-2-08> in cluster <MSUCluster> at Wed Oct 22 04:26:12 2025
</home_edu/edu-cmc-skmodel25-619-2/edu-cmc-skmodel25-619-2-08> was used as the home directory.
</home_edu/edu-cmc-skmodel25-619-2/edu-cmc-skmodel25-619-2-08/supercomputer> was used as the working directory.
Started at Wed Oct 22 04:26:12 2025
Terminated at Wed Oct 22 04:26:13 2025
Results reported at Wed Oct 22 04:26:13 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpiexec ./task_mpi 40 40
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.30 sec.
    Max Memory :                                 1 MB
    Average Memory :                             1.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              1
    Max Threads :                                1
    Run time :                                   1 sec.
    Turnaround time :                            1 sec.

The output (if any) follows:

==== Domain Decomposition Summary ====
Global grid: 40 x 40
Total processes: 2    Grid layout: 2 x 1
X partitions (2):    0  20
Y partitions (1):    0
======================================
[Rank  1] coords=(1,0) x:[  20,  40] y:[   0,  40] 
[Rank  0] coords=(0,0) x:[   0,  19] y:[   0,  40] 
[OK] Converged in 108 iterations, residual = 9.27443e-05
Total time: 0.0368459 seconds.
Saved solution to solution/solution_M_40_N_40.csv



PS:

Read file <log/stage_5/mpi_40_40_2_cerr.log> for stderr output of this job.

